{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Установка JVM"
      ],
      "metadata": {
        "id": "v_CI8Yvuu2Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "QQcrzPhCqPEr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OiwnH8bDhdU0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get upgrade\n",
        "sudo apt-get install default-jre\n",
        "#!java -version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Константы и переменные ноутбука"
      ],
      "metadata": {
        "id": "1jVkkauj3H9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/content/spark\"\n",
        "os.environ[\"PROJECT_HOME\"] = \"/content/TestProject\"\n",
        "\n",
        "PROJECT_FOLDER = \"/content/TestProject\""
      ],
      "metadata": {
        "id": "OvmZUyokqRSy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка Apache Spark"
      ],
      "metadata": {
        "id": "dKuMaXV1u77-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3-scala2.13.tgz\n",
        "tar -xzf ./spark-3.4.1-bin-hadoop3-scala2.13.tgz\n",
        "mv ./spark-3.4.1-bin-hadoop3-scala2.13 ${SPARK_HOME}\n",
        "rm ./spark-3.4.1-bin-hadoop3-scala2.13.tgz"
      ],
      "metadata": {
        "id": "M35zwV69iIjK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проверка версии Apache Spark и Scala"
      ],
      "metadata": {
        "id": "T3NLsLLFvN4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "${SPARK_HOME}/bin/spark-shell --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDyZ401svVD8",
        "outputId": "e075bce2-8e01-45f7-a524-71d38dc8903c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.13.8, OpenJDK 64-Bit Server VM, 11.0.20.1\n",
            "Branch HEAD\n",
            "Compiled by user centos on 2023-06-19T22:21:01Z\n",
            "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание структуры папок/файлов проекта"
      ],
      "metadata": {
        "id": "u82mdjeRvrKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p ${PROJECT_HOME}/src/{main,test}/{java,resources,scala}\n",
        "mkdir ${PROJECT_HOME}/{dataset,project,target}"
      ],
      "metadata": {
        "id": "PxsPzWSdkbnn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "touch ${PROJECT_HOME}/build.sbt\n",
        "touch ${PROJECT_HOME}/src/main/scala/SimpleApp.scala"
      ],
      "metadata": {
        "id": "iQlSlXScqXo2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Заполнение файлов build.sbt и SimpleApp.scala"
      ],
      "metadata": {
        "id": "kuhkU6WvwH9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Содержимое файла build.sbt\n",
        "cat >${PROJECT_HOME}/build.sbt <<EOL\n",
        "name := \"Simple Project\"\n",
        "\n",
        "version := \"1.0\"\n",
        "\n",
        "scalaVersion := \"2.13.8\"\n",
        "\n",
        "val sparkVersion = \"3.4.1\"\n",
        "\n",
        "libraryDependencies ++= Seq(\n",
        "  \"org.apache.spark\" %% \"spark-core\" % sparkVersion,\n",
        "  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion\n",
        ")\n",
        "EOL"
      ],
      "metadata": {
        "id": "eVQnllcEuUS5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Содержимое файла SimpleApp.scala\n",
        "cat >${PROJECT_HOME}/src/main/scala/SimpleApp.scala <<EOL\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{StructField, StructType}\n",
        "import org.apache.spark.sql.types.{StringType,  IntegerType, DateType}\n",
        "import org.apache.spark.sql.Column\n",
        "import org.apache.spark.sql.functions.{udf, col, lit, round, when}\n",
        "import org.apache.spark.sql.functions.{sum, count, avg, min, max}\n",
        "\n",
        "object SimpleApp extends App{\n",
        "\n",
        "    // Загрузка датасета для анализа\n",
        "    val pathFile = \"${PROJECT_HOME}/dataset/data.csv\"\n",
        "    val spark = SparkSession.builder\n",
        "                            .master(\"local[*]\")\n",
        "                            .appName(\"Simple Application\")\n",
        "                            .getOrCreate()\n",
        "\n",
        "    val schemaDfSalary = new StructType()\n",
        "      .add(\"employee\", StringType, true)\n",
        "      .add(\"age\", IntegerType, true)\n",
        "      .add(\"department\", StringType, true)\n",
        "      .add(\"state\", StringType, true)\n",
        "      .add(\"salary\", IntegerType, true)\n",
        "      .add(\"bonus\", IntegerType, true)\n",
        "\n",
        "    val dfSalary = spark.read.format(\"csv\")\n",
        "      .option(\"header\", \"true\")\n",
        "      .options(Map(\"delimiter\"->\",\"))\n",
        "      .schema(schemaDfSalary)\n",
        "      .load(pathFile).cache()\n",
        "\n",
        "    val dataCluster = Seq((\"GA\",\"cluster1\"),\n",
        "                          (\"AZ\",\"cluster1\"),\n",
        "                          (\"FL\",\"cluster1\"),\n",
        "                          (\"CA\",\"cluster2\"),\n",
        "                          (\"NY\",\"cluster2\"),\n",
        "                          (\"TX\",\"cluster2\"))\n",
        "\n",
        "    val columnsCluster = Seq(\"state_code\",\"cluster\")\n",
        "\n",
        "    val dfCluster = spark.createDataFrame(dataCluster).toDF(columnsCluster:_*)\n",
        "\n",
        "    val dfSalaryJoin = dfSalary.join(dfCluster,\n",
        "                                     dfSalary(\"state\") ===  dfCluster(\"state_code\"),\n",
        "                                     \"left\")\n",
        "\n",
        "    val dfSalarySelect = dfSalaryJoin.select(col(\"cluster\"),\n",
        "                                             col(\"department\"),\n",
        "                                             col(\"salary\"),\n",
        "                                             col(\"bonus\"))\n",
        "\n",
        "    val dfSalaryTotal = dfSalarySelect.withColumn(\"total_salary\",\n",
        "                        round(dfSalarySelect(\"salary\") + dfSalarySelect(\"salary\")/100*dfSalarySelect(\"bonus\"),2).cast(\"Integer\"))\n",
        "\n",
        "    val dfSalaryGroup = dfSalaryTotal.groupBy(\"cluster\",\"department\")\n",
        "                                     .agg(sum(\"total_salary\").as(\"sum_salary\"),\n",
        "                                          avg(\"total_salary\").as(\"avg_salary\"),\n",
        "                                          min(\"total_salary\").as(\"min_salary\"),\n",
        "                                          max(\"total_salary\").as(\"max_salary\"))\n",
        "\n",
        "    val dfSalaryGroupFilter = dfSalaryGroup\n",
        "                                      .where(dfSalaryGroup(\"department\")===\"Finance\" && dfSalaryGroup(\"sum_salary\") >= 100000)\n",
        "    println(\"*****\")\n",
        "    println(dfSalaryGroupFilter.show(false))\n",
        "    println(\"*****\")\n",
        "\n",
        "    spark.stop()\n",
        "  }\n",
        "\n",
        "EOL"
      ],
      "metadata": {
        "id": "IBAakqpG0nEQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка SBT"
      ],
      "metadata": {
        "id": "GpltikZcwpNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" | sudo tee /etc/apt/sources.list.d/sbt.list\n",
        "echo \"deb https://repo.scala-sbt.org/scalasbt/debian /\" | sudo tee /etc/apt/sources.list.d/sbt_old.list\n",
        "curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823\" | sudo apt-key add\n",
        "sudo apt-get update\n",
        "sudo apt-get install sbt"
      ],
      "metadata": {
        "id": "QymH4XaPIKSm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Смена рабочей папки"
      ],
      "metadata": {
        "id": "f7x_Ua4-xR3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(PROJECT_FOLDER)"
      ],
      "metadata": {
        "id": "vwbZ-3LsxW4_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сборка"
      ],
      "metadata": {
        "id": "SYGh2RylxnQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sbt package"
      ],
      "metadata": {
        "id": "X7C8TrC_uBTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод результата"
      ],
      "metadata": {
        "id": "t7ucIz46yuXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "${SPARK_HOME}/bin/spark-submit \\\n",
        "                --class \"SimpleApp\" \\\n",
        "                --master local[*] \\\n",
        "                ${PROJECT_HOME}/target/scala-2.13/simple-project_2.13-1.0.jar"
      ],
      "metadata": {
        "id": "zPDxWyaapPEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}